# Tech Challenge 1 - API de Consulta de Livros

Projeto desenvolvido para o Tech Challenge da Fase 1 da P√≥s-Tech FIAP em Machine Learning Engineering.

## üìã Sobre o Projeto

API REST p√∫blica desenvolvida com FastAPI para gerenciamento e consulta de cat√°logo de livros. O projeto inclui funcionalidade completa de web scraping para coleta autom√°tica de dados do site [books.toscrape.com](https://books.toscrape.com/), armazenamento em banco de dados SQLite e disponibiliza√ß√£o via endpoints RESTful.

## üéØ Objetivos do Projeto

- Desenvolver um pipeline completo de extra√ß√£o, transforma√ß√£o e disponibiliza√ß√£o de dados
- Criar uma API p√∫blica escal√°vel e reus√°vel para futuros modelos de Machine Learning
- Implementar web scraping robusto com processamento ass√≠ncrono
- Fornecer endpoints RESTful bem documentados e testados

## üöÄ Tecnologias Utilizadas

- **FastAPI** - Framework web moderno e r√°pido para constru√ß√£o de APIs
- **SQLAlchemy** - ORM para gerenciamento do banco de dados
- **httpx** - Cliente HTTP ass√≠ncrono para web scraping
- **BeautifulSoup4** - Parser HTML para extra√ß√£o de dados
- **Pydantic** - Valida√ß√£o de dados e serializa√ß√£o
- **Uvicorn** - Servidor ASGI de alta performance
- **Pytest** - Framework de testes
- **uv** - Gerenciador moderno de depend√™ncias e ambientes virtuais Python

## üì¶ Instala√ß√£o e Configura√ß√£o

### Pr√©-requisitos

- Python 3.11 ou superior
- [uv](https://github.com/astral-sh/uv) instalado

### Passos de Instala√ß√£o

```bash
# 1. Clone o reposit√≥rio
git clone https://github.com/fiap-7MLET/tech-challenge-1.git
cd tech-challenge-1

# 2. Instale as depend√™ncias usando uv
uv sync

# 3. (Opcional) Ative o ambiente virtual
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate  # Windows
```

### Configura√ß√£o de Vari√°veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
DATABASE_URL=sqlite:///db.sqlite3
DEBUG=False
```

## üèÉ Como Executar

### Iniciar o Servidor de Desenvolvimento

```bash
uv run uvicorn src.app:app --host 0.0.0.0 --port 8000 --reload
```

A aplica√ß√£o estar√° dispon√≠vel em:
- **API**: http://localhost:8000
- **Documenta√ß√£o Swagger**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Popular o Banco de Dados

Antes de usar a API, popule o banco de dados com dados dos livros:

```bash
curl -X POST http://localhost:8000/scraping/trigger
```

Este comando ir√°:
1. **Iniciar o scraping em background** (retorna imediatamente com um `job_id`)
2. Fazer scraping de aproximadamente 1000 livros do site books.toscrape.com
3. Salvar os dados no banco de dados SQLite
4. Gerar um arquivo CSV em `data/books.csv`

Para acompanhar o progresso:

```bash
# Usando o job_id retornado
curl "http://localhost:8000/scraping/status?job_id=1"

# Ou verificar o √∫ltimo job
curl "http://localhost:8000/scraping/status"
```

## üîß Endpoints da API

### Endpoints Principais (Obrigat√≥rios)

#### Health Check
- **GET** `/health/` - Verifica o status da API e conectividade com o banco de dados

#### Livros
- **GET** `/books/` - Lista todos os livros com pagina√ß√£o
  - Query params: `page` (default: 1), `per_page` (default: 10)
  - Resposta inclui URLs de navega√ß√£o: `next`, `previous`
- **GET** `/books/{id}` - Retorna detalhes de um livro espec√≠fico
- **GET** `/books/search` - Busca livros por t√≠tulo e/ou categoria
  - Query params: `title`, `category`, `page`, `per_page`
  - Resposta inclui URLs de navega√ß√£o: `next`, `previous`

#### Categorias
- **GET** `/categories/` - Lista todas as categorias dispon√≠veis com pagina√ß√£o
  - Query params: `page` (default: 1), `per_page` (default: 10)
  - Resposta inclui URLs de navega√ß√£o: `next`, `previous`

#### Scraping (Ass√≠ncrono)
- **POST** `/scraping/trigger` - **Inicia** o processo de scraping em background (retorna imediatamente)
  - Resposta inclui `job_id` para acompanhamento
  - Previne execu√ß√£o de m√∫ltiplos jobs simult√¢neos
- **GET** `/scraping/status` - Retorna status do scraping e estat√≠sticas do banco de dados
  - Query params opcionais: `job_id` (para consultar job espec√≠fico)
  - Retorna informa√ß√µes do √∫ltimo job se `job_id` n√£o for fornecido
  - Inclui: status do job (pending/in_progress/completed/error), progresso, timestamps

### Endpoints Opcionais (B√¥nus)

#### Estat√≠sticas (N√£o Implementados)
- **GET** `/stats/overview` - Estat√≠sticas gerais da cole√ß√£o
- **GET** `/stats/categories` - Estat√≠sticas detalhadas por categoria

#### Livros Extras (N√£o Implementados)
- **GET** `/books/top-rated` - Livros com melhor avalia√ß√£o
- **GET** `/books/price-range` - Filtra livros por faixa de pre√ßo

#### Machine Learning (N√£o Implementados)
- **GET** `/ml/features` - Dados formatados para features de ML
- **GET** `/ml/training-data` - Dataset para treinamento
- **POST** `/ml/predictions` - Endpoint para receber predi√ß√µes

#### Autentica√ß√£o (N√£o Implementado)
- **POST** `/auth/register` - Registro de usu√°rio
- **POST** `/auth/login` - Login de usu√°rio
- **POST** `/auth/logout` - Logout de usu√°rio
- **POST** `/auth/refresh` - Renova√ß√£o de token

## üåê Exemplos de Uso

A API pode ser testada de duas formas: via **linha de comando (curl)** ou via **Swagger UI (interface gr√°fica)**. Recomendamos usar o Swagger UI para explora√ß√£o inicial, pois oferece documenta√ß√£o interativa e valida√ß√£o autom√°tica.

### üìñ Acessando a Documenta√ß√£o Interativa

**Swagger UI**: http://localhost:8000/docs
**ReDoc**: http://localhost:8000/redoc

---

### Verificar Status da API

**Via curl:**
```bash
curl http://localhost:8000/health/
```

**Via Swagger:**
1. Acesse http://localhost:8000/docs
2. Localize `GET /health/`
3. Clique em "Try it out" ‚Üí "Execute"
4. Visualize a resposta com status da API e conectividade do banco

---

### Listar Livros (com pagina√ß√£o)

**Via curl:**
```bash
curl "http://localhost:8000/books/?page=1&per_page=10"
```

**Via Swagger:**
1. Acesse http://localhost:8000/docs
2. Localize `GET /books/`
3. Clique em "Try it out"
4. Ajuste os par√¢metros:
   - `page`: 1
   - `per_page`: 10
5. Clique em "Execute"

**Resposta de exemplo:**
```json
{
  "data": [
    {
      "id": 1,
      "title": "A Light in the Attic",
      "price": "51.77",
      "rating": 3,
      "availability": true,
      "category": "Poetry",
      "image": "https://books.toscrape.com/media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg"
    }
  ],
  "page": 1,
  "per_page": 10,
  "total": 100,
  "pages": 10,
  "next": "http://localhost:8000/books/?page=2&per_page=10",
  "previous": null
}
```

---

### Buscar Livro por ID

**Via curl:**
```bash
curl "http://localhost:8000/books/1"
```

**Via Swagger:**
1. Acesse http://localhost:8000/docs
2. Localize `GET /books/{id}`
3. Clique em "Try it out"
4. Insira o `id` desejado (ex: 1)
5. Clique em "Execute"

**Resposta de exemplo:**
```json
{
  "id": 1,
  "title": "A Light in the Attic",
  "price": "51.77",
  "rating": 3,
  "availability": true,
  "category": "Poetry",
  "image": "https://books.toscrape.com/media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg"
}
```

---

### Buscar Livros por T√≠tulo

**Via curl:**
```bash
curl "http://localhost:8000/books/search?title=python&page=1&per_page=10"
```

**Via Swagger:**
1. Acesse http://localhost:8000/docs
2. Localize `GET /books/search`
3. Clique em "Try it out"
4. Preencha os par√¢metros:
   - `title`: "python"
   - `page`: 1
   - `per_page`: 10
5. Clique em "Execute"

**Resposta de exemplo:**
```json
{
  "data": [
    {
      "id": 23,
      "title": "Learning Python",
      "price": "30.23",
      "rating": 4,
      "availability": true,
      "category": "Programming",
      "image": "https://books.toscrape.com/media/cache/..."
    }
  ],
  "page": 1,
  "per_page": 10,
  "total": 15,
  "pages": 2,
  "next": "http://localhost:8000/books/search?title=python&per_page=10&page=2",
  "previous": null
}
```

---

### Buscar Livros por Categoria

**Via curl:**
```bash
curl "http://localhost:8000/books/search?category=Fiction&page=1&per_page=10"
```

**Via Swagger:**
1. Acesse http://localhost:8000/docs
2. Localize `GET /books/search`
3. Clique em "Try it out"
4. Preencha:
   - `category`: "Fiction"
   - `page`: 1
   - `per_page`: 10
5. Clique em "Execute"

---

### Listar Todas as Categorias

**Via curl:**
```bash
curl "http://localhost:8000/categories/?page=1&per_page=20"
```

**Via Swagger:**
1. Acesse http://localhost:8000/docs
2. Localize `GET /categories/`
3. Clique em "Try it out"
4. Ajuste:
   - `page`: 1
   - `per_page`: 20
5. Clique em "Execute"

**Resposta de exemplo:**
```json
{
  "data": [
    {"name": "Travel", "count": 11},
    {"name": "Mystery", "count": 32},
    {"name": "Historical Fiction", "count": 14}
  ],
  "page": 1,
  "per_page": 20,
  "total": 50,
  "pages": 3,
  "next": "http://localhost:8000/categories/?page=2&per_page=20",
  "previous": null
}
```

---

### Disparar Processo de Scraping (Ass√≠ncrono)

**Via curl:**
```bash
# Inicia o scraping (retorna imediatamente)
curl -X POST http://localhost:8000/scraping/trigger
```

**Via Swagger:**
1. Acesse http://localhost:8000/docs
2. Localize `POST /scraping/trigger`
3. Clique em "Try it out"
4. Clique em "Execute"
5. **Resposta √© imediata** - n√£o precisa aguardar

**Resposta de exemplo:**
```json
{
  "status": "started",
  "message": "Scraping iniciado em background",
  "job_id": 1,
  "check_status_url": "/scraping/status?job_id=1"
}
```

**Se j√° existir um job em andamento:**
```json
{
  "status": "already_running",
  "message": "J√° existe um job de scraping em andamento",
  "job_id": 1,
  "job_status": "in_progress"
}
```

---

### Verificar Status do Scraping

**Via curl:**
```bash
# Verifica status de um job espec√≠fico
curl "http://localhost:8000/scraping/status?job_id=1"

# Ou verifica o √∫ltimo job executado
curl "http://localhost:8000/scraping/status"
```

**Via Swagger:**
1. Acesse http://localhost:8000/docs
2. Localize `GET /scraping/status`
3. Clique em "Try it out"
4. (Opcional) Informe o `job_id`
5. Clique em "Execute"

**Resposta de exemplo (job em andamento):**
```json
{
  "database": {
    "total_books": 150,
    "total_categories": 12,
    "database_populated": true
  },
  "last_job": {
    "job_id": 1,
    "status": "in_progress",
    "started_at": "2025-11-02T20:18:22.229239",
    "completed_at": null,
    "books_scraped": null,
    "books_saved": null,
    "csv_file": null,
    "error_message": null
  }
}
```

**Resposta de exemplo (job conclu√≠do):**
```json
{
  "database": {
    "total_books": 999,
    "total_categories": 50,
    "database_populated": true
  },
  "last_job": {
    "job_id": 1,
    "status": "completed",
    "started_at": "2025-11-02T20:18:22.229239",
    "completed_at": "2025-11-02T20:18:52.755027",
    "books_scraped": 1000,
    "books_saved": 1000,
    "csv_file": "data/books.csv",
    "error_message": null
  }
}
```

**Resposta de exemplo (job com erro):**
```json
{
  "database": {
    "total_books": 0,
    "total_categories": 0,
    "database_populated": false
  },
  "last_job": {
    "job_id": 1,
    "status": "error",
    "started_at": "2025-11-02T20:18:22.229239",
    "completed_at": "2025-11-02T20:18:25.123456",
    "books_scraped": null,
    "books_saved": null,
    "csv_file": null,
    "error_message": "Connection timeout to books.toscrape.com"
  }
}
```

---

### üí° Dicas para Usar o Swagger UI

- **Schemas**: Role at√© o final da p√°gina do Swagger para ver todos os modelos de dados
- **Valida√ß√£o**: O Swagger valida automaticamente os tipos de dados antes de enviar
- **Exemplos**: Clique em "Schema" ao lado de "Example Value" para ver a estrutura completa
- **Download**: Baixe a especifica√ß√£o OpenAPI em http://localhost:8000/openapi.json
- **Autoriza√ß√£o**: Quando implementada autentica√ß√£o, use o bot√£o "Authorize" no topo

## üìÅ Estrutura do Projeto

```
tech-challenge-1/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas/          # Schemas Pydantic para valida√ß√£o
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ book.py
‚îÇ   ‚îú‚îÄ‚îÄ models/                # Modelos SQLAlchemy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ book.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ user.py
‚îÇ   ‚îú‚îÄ‚îÄ routes/                # Rotas da API (endpoints)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ book_routes.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ category_routes.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ health_routes.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scraping_routes.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stats_routes.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ml_routes.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ user_routes.py
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scraping/         # L√≥gica de web scraping
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ core.py       # Scraper ass√≠ncrono principal
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ file_handler.py  # Manipula√ß√£o de arquivos CSV
‚îÇ   ‚îú‚îÄ‚îÄ extensions.py          # Configura√ß√£o do banco de dados
‚îÇ   ‚îú‚îÄ‚îÄ conf.py                # Configura√ß√µes gerais
‚îÇ   ‚îî‚îÄ‚îÄ app.py                 # Aplica√ß√£o principal FastAPI
‚îú‚îÄ‚îÄ data/                      # Dados gerados (CSV)
‚îú‚îÄ‚îÄ migrations/                # Migrations do banco de dados
‚îú‚îÄ‚îÄ tests/                     # Testes automatizados
‚îú‚îÄ‚îÄ pyproject.toml            # Configura√ß√£o de depend√™ncias (uv/pip)
‚îú‚îÄ‚îÄ uv.lock                   # Lock file do uv
‚îú‚îÄ‚îÄ Tech_Challenge_API.postman_collection.json  # Cole√ß√£o Postman
‚îî‚îÄ‚îÄ README.md                 # Este arquivo
```

## üóÑÔ∏è Banco de Dados

### Modelo de Dados

O projeto utiliza SQLite como banco de dados com a seguinte estrutura:

**Tabela: books**
- `id` (Integer, PK) - Identificador √∫nico
- `title` (String) - T√≠tulo do livro (√∫nico)
- `price` (Numeric) - Pre√ßo do livro
- `rating` (Integer) - Avalia√ß√£o de 1 a 5
- `availability` (Boolean) - Disponibilidade em estoque
- `category` (String) - Categoria do livro
- `image` (String) - URL da imagem

**Tabela: scraping_jobs**
- `id` (Integer, PK) - Identificador √∫nico do job
- `status` (String) - Status do job (pending, in_progress, completed, error)
- `started_at` (DateTime) - Timestamp de in√≠cio do job
- `completed_at` (DateTime) - Timestamp de conclus√£o do job
- `books_scraped` (Integer) - N√∫mero de livros coletados
- `books_saved` (Integer) - N√∫mero de livros salvos no banco
- `error_message` (Text) - Mensagem de erro se o job falhou
- `csv_file` (String) - Caminho do arquivo CSV gerado

**Tabela: users** (estrutura criada, endpoints n√£o implementados)
- `id` (Integer, PK) - Identificador √∫nico
- `email` (String) - Email do usu√°rio (√∫nico)
- `password` (String) - Senha hash

### Gerenciamento do Banco

O banco de dados √© criado automaticamente na primeira execu√ß√£o em `db.sqlite3`.

## üï∑Ô∏è Web Scraping

### Caracter√≠sticas do Scraper

- **Background Processing**: Executa em background com FastAPI BackgroundTasks
- **Status Tracking**: Acompanhamento em tempo real do progresso via job tracking
- **Ass√≠ncrono**: Utiliza `httpx` e `asyncio` para m√°xima performance
- **Robusto**: Tratamento de erros e retry autom√°tico
- **Completo**: Extrai todos os campos necess√°rios (t√≠tulo, pre√ßo, rating, categoria, imagem)
- **Escal√°vel**: Processa m√∫ltiplas p√°ginas em paralelo
- **Logging**: Registra progresso e erros durante a execu√ß√£o
- **Preven√ß√£o de Duplicatas**: Impede execu√ß√£o de m√∫ltiplos jobs simult√¢neos

### Execu√ß√£o Ass√≠ncrona

O scraping √© executado de forma ass√≠ncrona, proporcionando:

1. **Resposta Imediata**: A API retorna instantaneamente com um `job_id` ao inv√©s de bloquear
2. **Acompanhamento de Progresso**: Consulte o status a qualquer momento via `/scraping/status?job_id=X`
3. **Estados do Job**:
   - `pending`: Job criado e aguardando in√≠cio
   - `in_progress`: Scraping em andamento
   - `completed`: Scraping finalizado com sucesso
   - `error`: Erro durante o scraping (com mensagem detalhada)
4. **Prote√ß√£o Contra Concorr√™ncia**: Sistema impede m√∫ltiplos jobs simult√¢neos
5. **Persist√™ncia**: Hist√≥rico de jobs mantido no banco de dados

### Fonte de Dados

- **URL**: https://books.toscrape.com/
- **Campos Extra√≠dos**:
  - T√≠tulo do livro
  - Pre√ßo (em libras)
  - Rating (1-5 estrelas)
  - Disponibilidade em estoque
  - Categoria
  - URL da imagem

## üß™ Testes

### Executar Testes

```bash
# Executar todos os testes
uv run pytest

# Executar com output detalhado
uv run pytest -v

# Executar com cobertura de c√≥digo
uv run pytest --cov=src --cov-report=html
```

### Cobertura de Testes

O projeto inclui testes para:
- ‚úÖ Rotas da API
- ‚úÖ Modelos de dados
- ‚úÖ Schemas Pydantic
- ‚úÖ Fun√ß√µes de scraping
- ‚úÖ Manipula√ß√£o de arquivos CSV

## üìä Cole√ß√£o Postman

Uma cole√ß√£o Postman completa est√° dispon√≠vel em `Tech_Challenge_API.postman_collection.json` com todos os endpoints configurados e exemplos de requisi√ß√µes.

### Importar no Postman

1. Abra o Postman
2. Clique em "Import"
3. Selecione o arquivo `Tech_Challenge_API.postman_collection.json`
4. A cole√ß√£o estar√° dispon√≠vel com todos os endpoints pr√©-configurados

## üèóÔ∏è Arquitetura e Pipeline de Dados

### Fluxo de Dados

```
[books.toscrape.com]
    ‚Üì (Web Scraping - httpx + BeautifulSoup)
[Dados Brutos]
    ‚Üì (Transforma√ß√£o e Limpeza)
[Dados Estruturados]
    ‚Üì (Armazenamento Dual)
    ‚îú‚Üí [SQLite Database] ‚Üí [FastAPI] ‚Üí [Endpoints REST] ‚Üí [Consumidores]
    ‚îî‚Üí [CSV File] ‚Üí [An√°lise/ML]
```

### Escalabilidade Futura

A arquitetura foi desenhada pensando em:
- **Modularidade**: Componentes independentes e reutiliz√°veis
- **Extensibilidade**: F√°cil adi√ß√£o de novos endpoints e funcionalidades
- **ML-Ready**: Estrutura preparada para integra√ß√£o com modelos de ML
- **Cache**: Possibilidade de adicionar camada de cache (Redis)
- **Queue**: Preparado para adicionar filas de processamento (Celery)

## üéì Cen√°rio de Uso para ML

Esta API foi desenvolvida pensando em servir como base para:
1. **Sistemas de Recomenda√ß√£o**: Dados estruturados de livros, categorias e ratings
2. **An√°lise de Pre√ßos**: Hist√≥rico e compara√ß√£o de pre√ßos
3. **Classifica√ß√£o de Texto**: Categoriza√ß√£o autom√°tica baseada em t√≠tulos
4. **Feature Engineering**: Endpoints preparados para exportar features

## üë• Equipe

Desenvolvido como parte do Tech Challenge - Fase 1
P√≥s-Tech FIAP - Machine Learning Engineering

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT.

---

**Observa√ß√£o**: Este projeto foi desenvolvido para fins educacionais como parte do Tech Challenge da FIAP.
