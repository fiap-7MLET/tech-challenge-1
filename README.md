# Tech Challenge 1 - API de Consulta de Livros
Projeto desenvolvido para o Tech Challenge da Fase 1 da PÃ³s-Tech FIAP em Machine Learning Engineering acessÃ­vel no RepositÃ³rio PÃºblico do Github em [https://github.com/fiap-7MLET/tech-challenge-1](https://github.com/fiap-7MLET/tech-challenge-1).

---

<details>
<summary><strong>ğŸ—‚ SUMÃRIO</strong></summary>

- [ğŸ‘¥ Equipe](#-equipe-)
- [ğŸ“‹ Sobre o Projeto](#-sobre-o-projeto-)
- [ğŸ¯ Objetivos do Projeto](#-objetivos-do-projeto-)
- [ğŸš€ Tecnologias Utilizadas](#-tecnologias-utilizadas-)
- [ğŸŒ API PÃºblica](#-api-pÃºblica-)
- [ğŸ’¾ EntregÃ¡veis Adicionais](#-entregÃ¡veis-adicionais-)
  - [ğŸ¥ ApresentaÃ§Ã£o em VÃ­deo](#-apresentaÃ§Ã£o-em-vÃ­deo-)
  - [ğŸï¸ ApresentaÃ§Ã£o de Slides](#ï¸-apresentaÃ§Ã£o-de-slides-)
  - [ğŸ“Š ColeÃ§Ã£o Postman](#-coleÃ§Ã£o-postman-)
- [ğŸ“¦ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o](#-instalaÃ§Ã£o-e-configuraÃ§Ã£o-)
- [ğŸƒ Como Executar](#-como-executar-)
- [ğŸ”§ Endpoints da API](#-endpoints-da-api-)
  - [Endpoints Principais (ObrigatÃ³rios)](#endpoints-principais-obrigatÃ³rios-)
  - [Endpoints Opcionais (BÃ´nus)](#endpoints-opcionais-bÃ´nus-)
- [ğŸŒ Exemplos de Uso](#-exemplos-de-uso-)
- [ğŸ“ Estrutura do Projeto](#-estrutura-do-projeto-)
- [ğŸ—„ï¸ Banco de Dados](#ï¸-banco-de-dados-)
- [ğŸ•·ï¸ Web Scraping](#ï¸-web-scraping-)
- [ğŸ§ª Testes](#-testes-)
- [ğŸ—ï¸ Arquitetura e Pipeline de Dados](#ï¸-arquitetura-e-pipeline-de-dados-)
- [ğŸ—ï¸ CI/CD](#cicd-)
- [ğŸ“ CenÃ¡rio de Uso para ML](#-cenÃ¡rio-de-uso-para-ml-)
- [ğŸ“„ LicenÃ§a](#-licenÃ§a-)
</details>

---
## ğŸ‘¥ Equipe [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

Desenvolvido como parte do Tech Challenge - Fase 1
PÃ³s-Tech FIAP - Machine Learning Engineering

| Nome Completo | RM | Google Skills | Github |
|---------------|----|--------------------|-------------|
| Allan Vital | RM369068 | [Badge](https://www.skills.google/public_profiles/6336cac1-9227-4eeb-969e-dedafd67c003) | [@vitallan](https://github.com/vitallan) |
| Beatriz MendonÃ§a | RM367076 | [Badge](https://www.skills.google/public_profiles/7a5f64c0-9f90-4302-afaa-2f6024bdae53) | [@beamendon](https://github.com/beamendon) |
| Fernando Nunes | RM368361 | [Badge](https://www.skills.google/public_profiles/eef5e153-41c5-497d-bfd5-8f69ab2f1883) | [@fernandoleitao](https://github.com/fernandoleitao) |
| Nhaiara Moura | RM368096 | [Badge](https://www.skills.google/public_profiles/f1ae1664-8b59-4214-bbf9-086a19dc8faa) | [@nhaiara](https://github.com/nhaiara) |
| Rafael Melazzo | RM368728 | [Badge](https://www.skills.google/public_profiles/86e31f1d-0273-482f-af15-00e559df7961) | [@rafaelmelazzo](https://github.com/rafaelmelazzo) |

## ğŸ“‹ Sobre o Projeto [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

API REST pÃºblica desenvolvida com FastAPI para gerenciamento e consulta de catÃ¡logo de livros. O projeto inclui funcionalidade completa de web scraping para coleta automÃ¡tica de dados do site [books.toscrape.com](https://books.toscrape.com/), armazenamento em banco de dados SQLite e disponibilizaÃ§Ã£o via endpoints RESTful.


## ğŸ¯ Objetivos do Projeto [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

- Desenvolver um pipeline completo de extraÃ§Ã£o, transformaÃ§Ã£o e disponibilizaÃ§Ã£o de dados
- Criar uma API pÃºblica escalÃ¡vel e reusÃ¡vel para futuros modelos de Machine Learning
- Implementar web scraping robusto com processamento assÃ­ncrono
- Fornecer endpoints RESTful bem documentados e testados
- Apresentar o projeto em vÃ­deo

## ğŸš€ Tecnologias Utilizadas [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

- **FastAPI** - Framework web moderno e rÃ¡pido para construÃ§Ã£o de APIs
- **SQLAlchemy** - ORM para gerenciamento do banco de dados
- **httpx** - Cliente HTTP assÃ­ncrono para web scraping
- **BeautifulSoup4** - Parser HTML para extraÃ§Ã£o de dados
- **Pydantic** - ValidaÃ§Ã£o de dados e serializaÃ§Ã£o
- **Uvicorn** - Servidor ASGI de alta performance
- **Pytest** - Framework de testes
- **uv** - Gerenciador moderno de dependÃªncias e ambientes virtuais Python

## ğŸŒ API PÃºblica [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

- [https://tech-challenge-1-7zyn.onrender.com/docs](https://tech-challenge-1-7zyn.onrender.com/docs)

## ğŸ’¾ EntregÃ¡veis Adicionais [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

- ### ğŸ¥ ApresentaÃ§Ã£o em VÃ­deo [â†‘](#tech-challenge-1---api-de-consulta-de-livros)
  - Veja os detalhes e explicaÃ§Ã£o sobre o nosso projeto na nossa [apresentaÃ§Ã£o em vÃ­deo](https://drive.google.com/file/d/1RqUlpd3zPc5sA--CU2jniGa7yYpYW_nc/view).

- ### ğŸï¸ ApresentaÃ§Ã£o de Slides [â†‘](#tech-challenge-1---api-de-consulta-de-livros)
  - Acesse os slides [aqui](https://docs.google.com/presentation/d/1HfFd1JdZnVFsXir5gKII77OlkZJQyAfkj0i3a_KLE_U/edit?usp=sharing).

- ### ğŸ“Š ColeÃ§Ã£o Postman [â†‘](#tech-challenge-1---api-de-consulta-de-livros)
  Uma coleÃ§Ã£o Postman completa estÃ¡ disponÃ­vel em `Tech_Challenge_API.postman_collection.json` com todos os endpoints configurados e exemplos de requisiÃ§Ãµes.
  #### Importar no Postman
    1. Abra o Postman
    2. Clique em "Import"
    3. Selecione o arquivo `Tech_Challenge_API.postman_collection.json`
    4. A coleÃ§Ã£o estarÃ¡ disponÃ­vel com todos os endpoints prÃ©-configurados

## ğŸ“¦ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

### PrÃ©-requisitos

- Python 3.11 ou superior
- [uv](https://github.com/astral-sh/uv) instalado

### Passos de InstalaÃ§Ã£o

```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/fiap-7MLET/tech-challenge-1.git
cd tech-challenge-1

# 2. Instale as dependÃªncias usando uv
uv sync

# 3. (Opcional) Ative o ambiente virtual
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate  # Windows
```

### ConfiguraÃ§Ã£o de VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
DATABASE_URL=sqlite:///db.sqlite3
DEBUG=False
```

## ğŸƒ Como Executar [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

### Aplicar Migrations do Banco de Dados

**IMPORTANTE**: Antes de iniciar a aplicaÃ§Ã£o pela primeira vez, vocÃª deve aplicar as migrations do banco de dados:

```bash
uv run alembic upgrade head
```

Este comando irÃ¡:
1. Criar o arquivo de banco de dados SQLite (`db.sqlite3`)
2. Criar todas as tabelas necessÃ¡rias (books, users, scraping_jobs)
3. Configurar o versionamento do esquema do banco

### Iniciar o Servidor de Desenvolvimento

```bash
uv run uvicorn src.app:app --host 0.0.0.0 --port 8000 --reload
```

A aplicaÃ§Ã£o estarÃ¡ disponÃ­vel em:
- **API**: http://localhost:8000
- **DocumentaÃ§Ã£o Swagger**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Popular o Banco de Dados

Antes de usar a API, popule o banco de dados com dados dos livros:

```bash
curl -X POST http://localhost:8000/scraping/trigger
```

Este comando irÃ¡:
1. **Iniciar o scraping em background** (retorna imediatamente com um `job_id`)
2. Fazer scraping de aproximadamente 1000 livros do site books.toscrape.com
3. Salvar os dados no banco de dados SQLite
4. Gerar um arquivo CSV em `data/books.csv`

Para acompanhar o progresso:

```bash
# Usando o job_id retornado
curl "http://localhost:8000/scraping/status?job_id=1"

# Ou verificar o Ãºltimo job
curl "http://localhost:8000/scraping/status"
```

## ğŸ”§ Endpoints da API [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

### Endpoints Principais (ObrigatÃ³rios) [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

#### Health Check
- **GET** `/health/` - Verifica o status da API e conectividade com o banco de dados

#### Livros
- **GET** `/books/` - Lista todos os livros com paginaÃ§Ã£o
  - Query params: `page` (default: 1), `per_page` (default: 10)
  - Resposta inclui URLs de navegaÃ§Ã£o: `next`, `previous`
- **GET** `/books/{id}` - Retorna detalhes de um livro especÃ­fico
- **GET** `/books/search` - Busca livros por tÃ­tulo e/ou categoria
  - Query params: `title`, `category`, `page`, `per_page`
  - Resposta inclui URLs de navegaÃ§Ã£o: `next`, `previous`

#### Categorias
- **GET** `/categories/` - Lista todas as categorias disponÃ­veis com paginaÃ§Ã£o
  - Query params: `page` (default: 1), `per_page` (default: 10)
  - Resposta inclui URLs de navegaÃ§Ã£o: `next`, `previous`

#### Scraping (AssÃ­ncrono)
- **POST** `/scraping/trigger` - **Inicia** o processo de scraping em background (retorna imediatamente)
  - Resposta inclui `job_id` para acompanhamento
  - Previne execuÃ§Ã£o de mÃºltiplos jobs simultÃ¢neos
- **GET** `/scraping/status` - Retorna status do scraping e estatÃ­sticas do banco de dados
  - Query params opcionais: `job_id` (para consultar job especÃ­fico)
  - Retorna informaÃ§Ãµes do Ãºltimo job se `job_id` nÃ£o for fornecido
  - Inclui: status do job (pending/in_progress/completed/error), progresso, timestamps

### Endpoints Opcionais (BÃ´nus) [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

#### EstatÃ­sticas (NÃ£o Implementados)
- **GET** `/stats/overview` - EstatÃ­sticas gerais da coleÃ§Ã£o
- **GET** `/stats/categories` - EstatÃ­sticas detalhadas por categoria

#### Livros Extras (NÃ£o Implementados)
- **GET** `/books/top-rated` - Livros com melhor avaliaÃ§Ã£o
- **GET** `/books/price-range` - Filtra livros por faixa de preÃ§o

#### Machine Learning (NÃ£o Implementados)
- **GET** `/ml/features` - Dados formatados para features de ML
- **GET** `/ml/training-data` - Dataset para treinamento
- **POST** `/ml/predictions` - Endpoint para receber prediÃ§Ãµes

#### AutenticaÃ§Ã£o (NÃ£o Implementado)
- **POST** `/auth/register` - Registro de usuÃ¡rio
- **POST** `/auth/login` - Login de usuÃ¡rio
- **POST** `/auth/logout` - Logout de usuÃ¡rio
- **POST** `/auth/refresh` - RenovaÃ§Ã£o de token

## ğŸŒ Exemplos de Uso [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

A API pode ser testada de duas formas: via **linha de comando (curl)** ou via **Swagger UI (interface grÃ¡fica)**. Recomendamos usar o Swagger UI para exploraÃ§Ã£o inicial, pois oferece documentaÃ§Ã£o interativa e validaÃ§Ã£o automÃ¡tica.

### ğŸ“– Acessando a DocumentaÃ§Ã£o Interativa [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

**Swagger UI**: http://localhost:8000/docs
**ReDoc**: http://localhost:8000/redoc

---

### Verificar Status da API [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

**Via curl:**
```bash
curl http://localhost:8000/health/
```

**Via Swagger:**
1. Acesse http://localhost:8000/docs
2. Localize `GET /health/`
3. Clique em "Try it out" â†’ "Execute"
4. Visualize a resposta com status da API e conectividade do banco

---

### Listar Livros (com paginaÃ§Ã£o) [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

**Via curl:**
```bash
curl "http://localhost:8000/books/?page=1&per_page=10"
```

**Via Swagger:**
1. Acesse http://localhost:8000/docs
2. Localize `GET /books/`
3. Clique em "Try it out"
4. Ajuste os parÃ¢metros:
   - `page`: 1
   - `per_page`: 10
5. Clique em "Execute"

**Resposta de exemplo:**
```json
{
  "data": [
    {
      "id": 1,
      "title": "A Light in the Attic",
      "price": "51.77",
      "rating": 3,
      "availability": true,
      "category": "Poetry",
      "image": "https://books.toscrape.com/media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg"
    }
  ],
  "page": 1,
  "per_page": 10,
  "total": 100,
  "pages": 10,
  "next": "http://localhost:8000/books/?page=2&per_page=10",
  "previous": null
}
```

---

### Buscar Livro por ID [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

**Via curl:**
```bash
curl "http://localhost:8000/books/1"
```

**Via Swagger:**
1. Acesse http://localhost:8000/docs
2. Localize `GET /books/{id}`
3. Clique em "Try it out"
4. Insira o `id` desejado (ex: 1)
5. Clique em "Execute"

**Resposta de exemplo:**
```json
{
  "id": 1,
  "title": "A Light in the Attic",
  "price": "51.77",
  "rating": 3,
  "availability": true,
  "category": "Poetry",
  "image": "https://books.toscrape.com/media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg"
}
```

---

### Buscar Livros por TÃ­tulo [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

**Via curl:**
```bash
curl "http://localhost:8000/books/search?title=python&page=1&per_page=10"
```

**Via Swagger:**
1. Acesse http://localhost:8000/docs
2. Localize `GET /books/search`
3. Clique em "Try it out"
4. Preencha os parÃ¢metros:
   - `title`: "python"
   - `page`: 1
   - `per_page`: 10
5. Clique em "Execute"

**Resposta de exemplo:**
```json
{
  "data": [
    {
      "id": 23,
      "title": "Learning Python",
      "price": "30.23",
      "rating": 4,
      "availability": true,
      "category": "Programming",
      "image": "https://books.toscrape.com/media/cache/..."
    }
  ],
  "page": 1,
  "per_page": 10,
  "total": 15,
  "pages": 2,
  "next": "http://localhost:8000/books/search?title=python&per_page=10&page=2",
  "previous": null
}
```

---

### Buscar Livros por Categoria [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

**Via curl:**
```bash
curl "http://localhost:8000/books/search?category=Fiction&page=1&per_page=10"
```

**Via Swagger:**
1. Acesse http://localhost:8000/docs
2. Localize `GET /books/search`
3. Clique em "Try it out"
4. Preencha:
   - `category`: "Fiction"
   - `page`: 1
   - `per_page`: 10
5. Clique em "Execute"

---

### Listar Todas as Categorias [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

**Via curl:**
```bash
curl "http://localhost:8000/categories/?page=1&per_page=20"
```

**Via Swagger:**
1. Acesse http://localhost:8000/docs
2. Localize `GET /categories/`
3. Clique em "Try it out"
4. Ajuste:
   - `page`: 1
   - `per_page`: 20
5. Clique em "Execute"

**Resposta de exemplo:**
```json
{
  "data": [
    {"name": "Travel", "count": 11},
    {"name": "Mystery", "count": 32},
    {"name": "Historical Fiction", "count": 14}
  ],
  "page": 1,
  "per_page": 20,
  "total": 50,
  "pages": 3,
  "next": "http://localhost:8000/categories/?page=2&per_page=20",
  "previous": null
}
```

---

### Disparar Processo de Scraping (AssÃ­ncrono) [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

**Via curl:**
```bash
# Inicia o scraping (retorna imediatamente)
curl -X POST http://localhost:8000/scraping/trigger
```

**Via Swagger:**
1. Acesse http://localhost:8000/docs
2. Localize `POST /scraping/trigger`
3. Clique em "Try it out"
4. Clique em "Execute"
5. **Resposta Ã© imediata** - nÃ£o precisa aguardar

**Resposta de exemplo:**
```json
{
  "status": "started",
  "message": "Scraping iniciado em background",
  "job_id": 1,
  "check_status_url": "/scraping/status?job_id=1"
}
```

**Se jÃ¡ existir um job em andamento:**
```json
{
  "status": "already_running",
  "message": "JÃ¡ existe um job de scraping em andamento",
  "job_id": 1,
  "job_status": "in_progress"
}
```

---

### Verificar Status do Scraping [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

**Via curl:**
```bash
# Verifica status de um job especÃ­fico
curl "http://localhost:8000/scraping/status?job_id=1"

# Ou verifica o Ãºltimo job executado
curl "http://localhost:8000/scraping/status"
```

**Via Swagger:**
1. Acesse http://localhost:8000/docs
2. Localize `GET /scraping/status`
3. Clique em "Try it out"
4. (Opcional) Informe o `job_id`
5. Clique em "Execute"

**Resposta de exemplo (job em andamento):**
```json
{
  "database": {
    "total_books": 150,
    "total_categories": 12,
    "database_populated": true
  },
  "last_job": {
    "job_id": 1,
    "status": "in_progress",
    "started_at": "2025-11-02T20:18:22.229239",
    "completed_at": null,
    "books_scraped": null,
    "books_saved": null,
    "csv_file": null,
    "error_message": null
  }
}
```

**Resposta de exemplo (job concluÃ­do):**
```json
{
  "database": {
    "total_books": 999,
    "total_categories": 50,
    "database_populated": true
  },
  "last_job": {
    "job_id": 1,
    "status": "completed",
    "started_at": "2025-11-02T20:18:22.229239",
    "completed_at": "2025-11-02T20:18:52.755027",
    "books_scraped": 1000,
    "books_saved": 1000,
    "csv_file": "data/books.csv",
    "error_message": null
  }
}
```

**Resposta de exemplo (job com erro):**
```json
{
  "database": {
    "total_books": 0,
    "total_categories": 0,
    "database_populated": false
  },
  "last_job": {
    "job_id": 1,
    "status": "error",
    "started_at": "2025-11-02T20:18:22.229239",
    "completed_at": "2025-11-02T20:18:25.123456",
    "books_scraped": null,
    "books_saved": null,
    "csv_file": null,
    "error_message": "Connection timeout to books.toscrape.com"
  }
}
```

---

### ğŸ’¡ Dicas para Usar o Swagger UI [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

- **Schemas**: Role atÃ© o final da pÃ¡gina do Swagger para ver todos os modelos de dados
- **ValidaÃ§Ã£o**: O Swagger valida automaticamente os tipos de dados antes de enviar
- **Exemplos**: Clique em "Schema" ao lado de "Example Value" para ver a estrutura completa
- **Download**: Baixe a especificaÃ§Ã£o OpenAPI em http://localhost:8000/openapi.json
- **AutorizaÃ§Ã£o**: Quando implementada autenticaÃ§Ã£o, use o botÃ£o "Authorize" no topo

## ğŸ“ Estrutura do Projeto [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

```
tech-challenge-1/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ schemas/          # Schemas Pydantic para validaÃ§Ã£o
â”‚   â”‚       â””â”€â”€ book.py
â”‚   â”œâ”€â”€ models/                # Modelos SQLAlchemy
â”‚   â”‚   â”œâ”€â”€ book.py
â”‚   â”‚   â””â”€â”€ user.py
â”‚   â”œâ”€â”€ routes/                # Rotas da API (endpoints)
â”‚   â”‚   â”œâ”€â”€ book_routes.py
â”‚   â”‚   â”œâ”€â”€ category_routes.py
â”‚   â”‚   â”œâ”€â”€ health_routes.py
â”‚   â”‚   â”œâ”€â”€ scraping_routes.py
â”‚   â”‚   â”œâ”€â”€ stats_routes.py
â”‚   â”‚   â”œâ”€â”€ ml_routes.py
â”‚   â”‚   â””â”€â”€ user_routes.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ scraping/         # LÃ³gica de web scraping
â”‚   â”‚       â”œâ”€â”€ core.py       # Scraper assÃ­ncrono principal
â”‚   â”‚       â””â”€â”€ file_handler.py  # ManipulaÃ§Ã£o de arquivos CSV
â”‚   â”œâ”€â”€ extensions.py          # ConfiguraÃ§Ã£o do banco de dados
â”‚   â”œâ”€â”€ conf.py                # ConfiguraÃ§Ãµes gerais
â”‚   â””â”€â”€ app.py                 # AplicaÃ§Ã£o principal FastAPI
â”œâ”€â”€ data/                      # Dados gerados (CSV)
â”œâ”€â”€ migrations/                # Migrations do banco de dados
â”œâ”€â”€ tests/                     # Testes automatizados
â”œâ”€â”€ pyproject.toml            # ConfiguraÃ§Ã£o de dependÃªncias (uv/pip)
â”œâ”€â”€ uv.lock                   # Lock file do uv
â”œâ”€â”€ Tech_Challenge_API.postman_collection.json  # ColeÃ§Ã£o Postman
â””â”€â”€ README.md                 # Este arquivo
```

## ğŸ—„ï¸ Banco de Dados [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

### Modelo de Dados

O projeto utiliza SQLite como banco de dados com a seguinte estrutura:

**Tabela: books**
- `id` (Integer, PK) - Identificador Ãºnico
- `title` (String) - TÃ­tulo do livro (Ãºnico)
- `price` (Numeric) - PreÃ§o do livro
- `rating` (Integer) - AvaliaÃ§Ã£o de 1 a 5
- `availability` (Boolean) - Disponibilidade em estoque
- `category` (String) - Categoria do livro
- `image` (String) - URL da imagem

**Tabela: scraping_jobs**
- `id` (Integer, PK) - Identificador Ãºnico do job
- `status` (String) - Status do job (pending, in_progress, completed, error)
- `started_at` (DateTime) - Timestamp de inÃ­cio do job
- `completed_at` (DateTime) - Timestamp de conclusÃ£o do job
- `books_scraped` (Integer) - NÃºmero de livros coletados
- `books_saved` (Integer) - NÃºmero de livros salvos no banco
- `error_message` (Text) - Mensagem de erro se o job falhou
- `csv_file` (String) - Caminho do arquivo CSV gerado

**Tabela: users** (estrutura criada, endpoints nÃ£o implementados)
- `id` (Integer, PK) - Identificador Ãºnico
- `email` (String) - Email do usuÃ¡rio (Ãºnico)
- `password` (String) - Senha hash

### Gerenciamento do Banco [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

O banco de dados Ã© gerenciado atravÃ©s do Alembic (migrations). Para criar ou atualizar o banco de dados, execute:

```bash
# Aplicar todas as migrations
uv run alembic upgrade head

# Verificar versÃ£o atual do banco
uv run alembic current

# Ver histÃ³rico de migrations
uv run alembic history
```

O arquivo do banco Ã© criado em `db.sqlite3` apÃ³s a primeira execuÃ§Ã£o das migrations.

## ğŸ•·ï¸ Web Scraping [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

### CaracterÃ­sticas do Scraper

- **Background Processing**: Executa em background com FastAPI BackgroundTasks
- **Status Tracking**: Acompanhamento em tempo real do progresso via job tracking
- **AssÃ­ncrono**: Utiliza `httpx` e `asyncio` para mÃ¡xima performance
- **Robusto**: Tratamento de erros e retry automÃ¡tico
- **Completo**: Extrai todos os campos necessÃ¡rios (tÃ­tulo, preÃ§o, rating, categoria, imagem)
- **EscalÃ¡vel**: Processa mÃºltiplas pÃ¡ginas em paralelo
- **Logging**: Registra progresso e erros durante a execuÃ§Ã£o
- **PrevenÃ§Ã£o de Duplicatas**: Impede execuÃ§Ã£o de mÃºltiplos jobs simultÃ¢neos

### ExecuÃ§Ã£o AssÃ­ncrona

O scraping Ã© executado de forma assÃ­ncrona, proporcionando:

1. **Resposta Imediata**: A API retorna instantaneamente com um `job_id` ao invÃ©s de bloquear
2. **Acompanhamento de Progresso**: Consulte o status a qualquer momento via `/scraping/status?job_id=X`
3. **Estados do Job**:
   - `pending`: Job criado e aguardando inÃ­cio
   - `in_progress`: Scraping em andamento
   - `completed`: Scraping finalizado com sucesso
   - `error`: Erro durante o scraping (com mensagem detalhada)
4. **ProteÃ§Ã£o Contra ConcorrÃªncia**: Sistema impede mÃºltiplos jobs simultÃ¢neos
5. **PersistÃªncia**: HistÃ³rico de jobs mantido no banco de dados

### Fonte de Dados [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

- **URL**: https://books.toscrape.com/
- **Campos ExtraÃ­dos**:
  - TÃ­tulo do livro
  - PreÃ§o (em libras)
  - Rating (1-5 estrelas)
  - Disponibilidade em estoque
  - Categoria
  - URL da imagem

## ğŸ§ª Testes [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

### Executar Testes

```bash
# Executar todos os testes
uv run pytest

# Executar com output detalhado
uv run pytest -v

# Executar com cobertura de cÃ³digo
uv run pytest --cov=src --cov-report=html
```

### Cobertura de Testes [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

O projeto inclui testes para:
- âœ… Rotas da API
- âœ… Modelos de dados
- âœ… Schemas Pydantic
- âœ… FunÃ§Ãµes de scraping
- âœ… ManipulaÃ§Ã£o de arquivos CSV

## ğŸ—ï¸ Arquitetura e Pipeline de Dados [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

### VisÃ£o Geral

A imagem ilustra a arquitetura lÃ³gica da API, destacando a separaÃ§Ã£o de responsabilidades entre as principais camadas do cÃ³digo e suas interaÃ§Ãµes com as fontes de dados.

<img src="docs/arquitetura-geral.jpg"><br/>

- **Routes**: define os endpoints da aplicaÃ§Ã£o, atua como ponto de entrada das requisiÃ§Ãµes HTTP e encaminha as chamadas para as demais camadas conforme o tipo de operaÃ§Ã£o solicitada.
- **Services**: centraliza a lÃ³gica de negÃ³cio da aplicaÃ§Ã£o, orquestra o fluxo entre as rotas, modelos e fontes de dados, e controla o processo de scraping, leitura e escrita no banco e no CSV.
- **Models**: representa as entidades do domÃ­nio do sistema, implementa o mapeamento objeto-relacional via SQLAlchemy e garante consistÃªncia entre os objetos da aplicaÃ§Ã£o e as tabelas do banco.
- **Schemas**: define os modelos de entrada e saÃ­da de dados da API com Pydantic, assegurando a validaÃ§Ã£o e serializaÃ§Ã£o das informaÃ§Ãµes trafegadas entre o cliente e o servidor.
- **Data Sources**: compreende as camadas de persistÃªncia da aplicaÃ§Ã£o, sendo o SQLite a principal fonte de dados usada para leitura e escrita, e o CSV o artefato auxiliar para futura integraÃ§Ã£o com pipelines de ciÃªncia de dados.

### CI/CD [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

A imagem representa o fluxo de IntegraÃ§Ã£o ContÃ­nua (CI) e Entrega ContÃ­nua (CD) da aplicaÃ§Ã£o, mostrando as etapas automatizadas que garantem a qualidade do cÃ³digo, o deploy da aplicaÃ§Ã£o e a execuÃ§Ã£o do scraping em ambiente de produÃ§Ã£o (Render). O fluxo de CI/CD conta com os seguintes workflows:

<img src="docs/arquitetura-cicd.jpg" width="70%"><br/>

- **Testes e Cobertura**: etapa inicial do pipeline acionada a cada push no branch principal (main). Nela, sÃ£o instaladas dependÃªncias, executados testes automatizados e gerado o relatÃ³rio de cobertura de cÃ³digo, que Ã© publicado como artefato para anÃ¡lise posterior.
- **Deploy**: responsÃ¡vel por acionar o processo de deploy automÃ¡tico no Render, utilizando um Deploy Hook URL configurado no repositÃ³rio. Essa etapa envia o commit hash do GitHub para vincular a versÃ£o implantada ao cÃ³digo-fonte correspondente. A seguir um polling de status aguarda o Deploy se concluÃ­do, para que entÃ£o o job que scraping possa ser acionado. O job de scraping Ã© responsÃ¡vel por executar o endpoint da API que realiza o scraping a persistencia dos dados obtidos, fazendo com que a API esteja pronta para utilizaÃ§Ã£o

### Fluxo Sequencial da API [â†‘](#tech-challenge-1---api-de-consulta-de-livros)
A imagem apresenta o diagrama de sequÃªncia da aplicaÃ§Ã£o Scraper API, descrevendo em detalhes os trÃªs principais fluxos que compÃµem o processo completo de scraping, verificaÃ§Ã£o e consumo dos dados.

<img src="docs/arquitetura-sequencia-scraping.jpg" width="70%"><br/>

- **Fluxo 1 - Scraping Trigger**: inicia o processo de extraÃ§Ã£o de dados. O cliente envia uma requisiÃ§Ã£o POST /scraping/trigger, acionando a API para criar um novo registro de Scraping Job no banco de dados, com status inicial pending. A partir disso, um processo assÃ­ncrono Ã© executado (scrap_books()), que realiza requisiÃ§Ãµes HTTP ao site books.toscrape.com para coletar as informaÃ§Ãµes de livros (tÃ­tulo, preÃ§o, categoria, disponibilidade, avaliaÃ§Ã£o e imagem). Os dados extraÃ­dos sÃ£o armazenados localmente no arquivo books.csv e simultaneamente persistidos no banco SQLite. Ao tÃ©rmino do processo, o registro do job Ã© atualizado com o status final e o nÃºmero de itens processados.
- **Fluxo 2 - Verifica Status do Scraping**: permite ao cliente acompanhar a execuÃ§Ã£o do scraping em tempo real. Por meio do endpoint GET /scraping/status?job_id={id}, a API consulta o banco e retorna os detalhes do job, como identificador, status atual (pending, in_progress ou completed), horÃ¡rio de inÃ­cio e tÃ©rmino, e possÃ­veis mensagens de erro. Esse fluxo possibilita monitorar o progresso sem bloquear o cliente enquanto o scraping Ã© processado em background.
- **Fluxo 3 - Consumo de Dados**: disponibiliza os resultados do scraping por meio do endpoint GET /books. Ao ser acionado, o serviÃ§o consulta o banco SQLite, recupera a lista de livros extraÃ­dos e retorna os dados em formato JSON padronizado. Esse fluxo representa a etapa final do pipeline, permitindo que cientistas de dados, aplicaÃ§Ãµes externas ou sistemas de recomendaÃ§Ã£o consumam as informaÃ§Ãµes atualizadas diretamente da API.

### Fluxo de Dados [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

```
[books.toscrape.com]
    â†“ (Web Scraping - httpx + BeautifulSoup)
[Dados Brutos]
    â†“ (TransformaÃ§Ã£o e Limpeza)
[Dados Estruturados]
    â†“ (Armazenamento Dual)
    â”œâ†’ [SQLite Database] â†’ [FastAPI] â†’ [Endpoints REST] â†’ [Consumidores]
    â””â†’ [CSV File] â†’ [AnÃ¡lise/ML]
```

### Escalabilidade Futura [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

A arquitetura foi desenhada pensando em:
- **Modularidade**: Componentes independentes e reutilizÃ¡veis
- **Extensibilidade**: FÃ¡cil adiÃ§Ã£o de novos endpoints e funcionalidades
- **ML-Ready**: Estrutura preparada para integraÃ§Ã£o com modelos de ML
- **Cache**: Possibilidade de adicionar camada de cache (Redis)
- **Queue**: Preparado para adicionar filas de processamento (Celery)

## ğŸ“ CenÃ¡rio de Uso para ML [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

Esta API foi desenvolvida pensando em servir como base para:
1. **Sistemas de RecomendaÃ§Ã£o**: Dados estruturados de livros, categorias e ratings
2. **AnÃ¡lise de PreÃ§os**: HistÃ³rico e comparaÃ§Ã£o de preÃ§os
3. **ClassificaÃ§Ã£o de Texto**: CategorizaÃ§Ã£o automÃ¡tica baseada em tÃ­tulos
4. **Feature Engineering**: Endpoints preparados para exportar features


## ğŸ“„ LicenÃ§a [â†‘](#tech-challenge-1---api-de-consulta-de-livros)

Este projeto estÃ¡ sob a licenÃ§a MIT.

---

**ObservaÃ§Ã£o**: Este projeto foi desenvolvido para fins educacionais como parte do Tech Challenge da PÃ³s GraduaÃ§Ã£o FIAP.
