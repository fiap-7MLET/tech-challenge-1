# Tech Challenge 1 - API de Consulta de Livros

Projeto desenvolvido para o Tech Challenge da Fase 1 da PÃ³s-Tech FIAP em Machine Learning Engineering.

## ğŸ“‹ Sobre o Projeto

API REST pÃºblica desenvolvida com FastAPI para gerenciamento e consulta de catÃ¡logo de livros. O projeto inclui funcionalidade completa de web scraping para coleta automÃ¡tica de dados do site [books.toscrape.com](https://books.toscrape.com/), armazenamento em banco de dados SQLite e disponibilizaÃ§Ã£o via endpoints RESTful.

## ğŸ¯ Objetivos do Projeto

- Desenvolver um pipeline completo de extraÃ§Ã£o, transformaÃ§Ã£o e disponibilizaÃ§Ã£o de dados
- Criar uma API pÃºblica escalÃ¡vel e reusÃ¡vel para futuros modelos de Machine Learning
- Implementar web scraping robusto com processamento assÃ­ncrono
- Fornecer endpoints RESTful bem documentados e testados

## ğŸš€ Tecnologias Utilizadas

- **FastAPI** - Framework web moderno e rÃ¡pido para construÃ§Ã£o de APIs
- **SQLAlchemy** - ORM para gerenciamento do banco de dados
- **httpx** - Cliente HTTP assÃ­ncrono para web scraping
- **BeautifulSoup4** - Parser HTML para extraÃ§Ã£o de dados
- **Pydantic** - ValidaÃ§Ã£o de dados e serializaÃ§Ã£o
- **Uvicorn** - Servidor ASGI de alta performance
- **Pytest** - Framework de testes
- **uv** - Gerenciador moderno de dependÃªncias e ambientes virtuais Python

## ğŸ“¦ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### PrÃ©-requisitos

- Python 3.11 ou superior
- [uv](https://github.com/astral-sh/uv) instalado

### Passos de InstalaÃ§Ã£o

```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/fiap-7MLET/tech-challenge-1.git
cd tech-challenge-1

# 2. Instale as dependÃªncias usando uv
uv sync

# 3. (Opcional) Ative o ambiente virtual
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate  # Windows
```

### ConfiguraÃ§Ã£o de VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
DATABASE_URL=sqlite:///db.sqlite3
DEBUG=False
```

## ğŸƒ Como Executar

### Iniciar o Servidor de Desenvolvimento

```bash
uv run uvicorn src.app:app --host 0.0.0.0 --port 8000 --reload
```

A aplicaÃ§Ã£o estarÃ¡ disponÃ­vel em:
- **API**: http://localhost:8000
- **DocumentaÃ§Ã£o Swagger**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Popular o Banco de Dados

Antes de usar a API, popule o banco de dados com dados dos livros:

```bash
curl -X POST http://localhost:8000/scraping/trigger
```

Este comando irÃ¡:
1. Fazer scraping de aproximadamente 1000 livros do site books.toscrape.com
2. Salvar os dados no banco de dados SQLite
3. Gerar um arquivo CSV em `data/books.csv`
4. Retornar estatÃ­sticas da operaÃ§Ã£o

## ğŸ”§ Endpoints da API

### Endpoints Principais (ObrigatÃ³rios)

#### Health Check
- **GET** `/health/` - Verifica o status da API e conectividade com o banco de dados

#### Livros
- **GET** `/books/` - Lista todos os livros com paginaÃ§Ã£o
  - Query params: `page` (default: 1), `per_page` (default: 5)
- **GET** `/books/{id}` - Retorna detalhes de um livro especÃ­fico
- **GET** `/books/search` - Busca livros por tÃ­tulo e/ou categoria
  - Query params: `title`, `category`, `page`, `per_page`

#### Categorias
- **GET** `/categories/` - Lista todas as categorias disponÃ­veis com paginaÃ§Ã£o
  - Query params: `page` (default: 1), `per_page` (default: 5)

#### Scraping
- **POST** `/scraping/trigger` - Dispara o processo de scraping e popula o banco de dados
- **GET** `/scraping/status` - Retorna estatÃ­sticas do banco de dados (total de livros, categorias, etc.)

### Endpoints Opcionais (BÃ´nus)

#### EstatÃ­sticas (NÃ£o Implementados)
- **GET** `/stats/overview` - EstatÃ­sticas gerais da coleÃ§Ã£o
- **GET** `/stats/categories` - EstatÃ­sticas detalhadas por categoria

#### Livros Extras (NÃ£o Implementados)
- **GET** `/books/top-rated` - Livros com melhor avaliaÃ§Ã£o
- **GET** `/books/price-range` - Filtra livros por faixa de preÃ§o

#### Machine Learning (NÃ£o Implementados)
- **GET** `/ml/features` - Dados formatados para features de ML
- **GET** `/ml/training-data` - Dataset para treinamento
- **POST** `/ml/predictions` - Endpoint para receber prediÃ§Ãµes

#### AutenticaÃ§Ã£o (NÃ£o Implementado)
- **POST** `/auth/register` - Registro de usuÃ¡rio
- **POST** `/auth/login` - Login de usuÃ¡rio
- **POST** `/auth/logout` - Logout de usuÃ¡rio
- **POST** `/auth/refresh` - RenovaÃ§Ã£o de token

## ğŸŒ Exemplos de Uso

### Verificar Status da API

```bash
curl http://localhost:8000/health/
```

### Listar Livros (com paginaÃ§Ã£o)

```bash
curl "http://localhost:8000/books/?page=1&per_page=10"
```

### Buscar Livro por ID

```bash
curl "http://localhost:8000/books/1"
```

### Buscar Livros por TÃ­tulo

```bash
curl "http://localhost:8000/books/search?title=python&page=1&per_page=5"
```

### Buscar Livros por Categoria

```bash
curl "http://localhost:8000/books/search?category=Fiction&page=1&per_page=5"
```

### Listar Todas as Categorias

```bash
curl "http://localhost:8000/categories/?page=1&per_page=20"
```

### Verificar Status do Scraping

```bash
curl "http://localhost:8000/scraping/status"
```

## ğŸ“ Estrutura do Projeto

```
tech-challenge-1/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ schemas/          # Schemas Pydantic para validaÃ§Ã£o
â”‚   â”‚       â””â”€â”€ book.py
â”‚   â”œâ”€â”€ models/                # Modelos SQLAlchemy
â”‚   â”‚   â”œâ”€â”€ book.py
â”‚   â”‚   â””â”€â”€ user.py
â”‚   â”œâ”€â”€ routes/                # Rotas da API (endpoints)
â”‚   â”‚   â”œâ”€â”€ book_routes.py
â”‚   â”‚   â”œâ”€â”€ category_routes.py
â”‚   â”‚   â”œâ”€â”€ health_routes.py
â”‚   â”‚   â”œâ”€â”€ scraping_routes.py
â”‚   â”‚   â”œâ”€â”€ stats_routes.py
â”‚   â”‚   â”œâ”€â”€ ml_routes.py
â”‚   â”‚   â””â”€â”€ user_routes.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ scraping/         # LÃ³gica de web scraping
â”‚   â”‚       â”œâ”€â”€ core.py       # Scraper assÃ­ncrono principal
â”‚   â”‚       â””â”€â”€ file_handler.py  # ManipulaÃ§Ã£o de arquivos CSV
â”‚   â”œâ”€â”€ extensions.py          # ConfiguraÃ§Ã£o do banco de dados
â”‚   â”œâ”€â”€ conf.py                # ConfiguraÃ§Ãµes gerais
â”‚   â””â”€â”€ app.py                 # AplicaÃ§Ã£o principal FastAPI
â”œâ”€â”€ data/                      # Dados gerados (CSV)
â”œâ”€â”€ migrations/                # Migrations do banco de dados
â”œâ”€â”€ tests/                     # Testes automatizados
â”œâ”€â”€ pyproject.toml            # ConfiguraÃ§Ã£o de dependÃªncias (uv/pip)
â”œâ”€â”€ uv.lock                   # Lock file do uv
â”œâ”€â”€ Tech_Challenge_API.postman_collection.json  # ColeÃ§Ã£o Postman
â””â”€â”€ README.md                 # Este arquivo
```

## ğŸ—„ï¸ Banco de Dados

### Modelo de Dados

O projeto utiliza SQLite como banco de dados com a seguinte estrutura:

**Tabela: books**
- `id` (Integer, PK) - Identificador Ãºnico
- `title` (String) - TÃ­tulo do livro (Ãºnico)
- `price` (Numeric) - PreÃ§o do livro
- `rating` (Integer) - AvaliaÃ§Ã£o de 1 a 5
- `availability` (Boolean) - Disponibilidade em estoque
- `category` (String) - Categoria do livro
- `image` (String) - URL da imagem

### Gerenciamento do Banco

O banco de dados Ã© criado automaticamente na primeira execuÃ§Ã£o em `db.sqlite3`.

## ğŸ•·ï¸ Web Scraping

### CaracterÃ­sticas do Scraper

- **AssÃ­ncrono**: Utiliza `httpx` e `asyncio` para mÃ¡xima performance
- **Robusto**: Tratamento de erros e retry automÃ¡tico
- **Completo**: Extrai todos os campos necessÃ¡rios (tÃ­tulo, preÃ§o, rating, categoria, imagem)
- **EscalÃ¡vel**: Processa mÃºltiplas pÃ¡ginas em paralelo
- **Logging**: Registra progresso e erros durante a execuÃ§Ã£o

### Fonte de Dados

- **URL**: https://books.toscrape.com/
- **Campos ExtraÃ­dos**:
  - TÃ­tulo do livro
  - PreÃ§o (em libras)
  - Rating (1-5 estrelas)
  - Disponibilidade em estoque
  - Categoria
  - URL da imagem

## ğŸ§ª Testes

### Executar Testes

```bash
# Executar todos os testes
uv run pytest

# Executar com output detalhado
uv run pytest -v

# Executar com cobertura de cÃ³digo
uv run pytest --cov=src --cov-report=html
```

### Cobertura de Testes

O projeto inclui testes para:
- âœ… Rotas da API
- âœ… Modelos de dados
- âœ… Schemas Pydantic
- âœ… FunÃ§Ãµes de scraping
- âœ… ManipulaÃ§Ã£o de arquivos CSV

## ğŸ“Š ColeÃ§Ã£o Postman

Uma coleÃ§Ã£o Postman completa estÃ¡ disponÃ­vel em `Tech_Challenge_API.postman_collection.json` com todos os endpoints configurados e exemplos de requisiÃ§Ãµes.

### Importar no Postman

1. Abra o Postman
2. Clique em "Import"
3. Selecione o arquivo `Tech_Challenge_API.postman_collection.json`
4. A coleÃ§Ã£o estarÃ¡ disponÃ­vel com todos os endpoints prÃ©-configurados

## ğŸ—ï¸ Arquitetura e Pipeline de Dados

### Fluxo de Dados

```
[books.toscrape.com]
    â†“ (Web Scraping - httpx + BeautifulSoup)
[Dados Brutos]
    â†“ (TransformaÃ§Ã£o e Limpeza)
[Dados Estruturados]
    â†“ (Armazenamento Dual)
    â”œâ†’ [SQLite Database] â†’ [FastAPI] â†’ [Endpoints REST] â†’ [Consumidores]
    â””â†’ [CSV File] â†’ [AnÃ¡lise/ML]
```

### Escalabilidade Futura

A arquitetura foi desenhada pensando em:
- **Modularidade**: Componentes independentes e reutilizÃ¡veis
- **Extensibilidade**: FÃ¡cil adiÃ§Ã£o de novos endpoints e funcionalidades
- **ML-Ready**: Estrutura preparada para integraÃ§Ã£o com modelos de ML
- **Cache**: Possibilidade de adicionar camada de cache (Redis)
- **Queue**: Preparado para adicionar filas de processamento (Celery)

## ğŸ“ CenÃ¡rio de Uso para ML

Esta API foi desenvolvida pensando em servir como base para:
1. **Sistemas de RecomendaÃ§Ã£o**: Dados estruturados de livros, categorias e ratings
2. **AnÃ¡lise de PreÃ§os**: HistÃ³rico e comparaÃ§Ã£o de preÃ§os
3. **ClassificaÃ§Ã£o de Texto**: CategorizaÃ§Ã£o automÃ¡tica baseada em tÃ­tulos
4. **Feature Engineering**: Endpoints preparados para exportar features

## ğŸ‘¥ Equipe

Desenvolvido como parte do Tech Challenge - Fase 1
PÃ³s-Tech FIAP - Machine Learning Engineering

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT.

---

**ObservaÃ§Ã£o**: Este projeto foi desenvolvido para fins educacionais como parte do Tech Challenge da FIAP.
